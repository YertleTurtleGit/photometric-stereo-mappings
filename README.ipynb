{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- START doctoc -->\n",
    "<!-- END doctoc -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEOvTI9fVvIw"
   },
   "source": [
    "# Load Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the repository in your local environment you just need to run this once with ``already_downloaded`` set to ``False``.\n",
    "It will then download the test data automatically (into the folder ``TEST_DATA``). After that you can run this script with ``already_downloaded`` set to ``True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_zoFfhl9lID",
    "outputId": "abe177a7-8e32-4eb4-f8f7-b533e26500ef"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from utilities.image_file import read_image, show_image, show_images\n",
    "\n",
    "TOP_LIGHT_IMAGES_PATHS = []\n",
    "BOTTOM_LIGHT_IMAGES_PATHS = []\n",
    "for image_path in Path(\"test_data\").glob(\"*.png\"):\n",
    "    if \"TL\" in image_path.name:\n",
    "        TOP_LIGHT_IMAGES_PATHS.append(image_path)\n",
    "    elif \"BL\" in image_path.name:\n",
    "        BOTTOM_LIGHT_IMAGES_PATHS.append(image_path)\n",
    "\n",
    "TOP_LIGHT_IMAGES_PATHS = natsorted(TOP_LIGHT_IMAGES_PATHS, key=lambda y: y.name)\n",
    "BOTTOM_LIGHT_IMAGES_PATHS = natsorted(BOTTOM_LIGHT_IMAGES_PATHS, key=lambda y: y.name)\n",
    "\n",
    "TOP_LIGHT_IMAGES = [read_image(image_path) for image_path in TOP_LIGHT_IMAGES_PATHS]\n",
    "BOTTOM_LIGHT_IMAGES = [\n",
    "    read_image(image_path) for image_path in BOTTOM_LIGHT_IMAGES_PATHS\n",
    "]\n",
    "\n",
    "show_images(TOP_LIGHT_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdod0hvhWWaa"
   },
   "source": [
    "# Corrections & Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx6jo1p-Wd8A"
   },
   "source": [
    "## Lens Related Corrections\n",
    "\n",
    "Are applied per lens using lens specific calibration data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpmXqwXbW1D-"
   },
   "source": [
    "### Vignette Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t7VlhnSW6MQ"
   },
   "source": [
    "### Distortion Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaPngqqpW9AM"
   },
   "source": [
    "### Transversal Chromatic Aberration (TCA) Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxGqsVeqZDAk"
   },
   "source": [
    "## TDM Related Corrections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yja-e0DTZJ7w"
   },
   "source": [
    "### Image Alignment\n",
    "\n",
    "from bit operations on median threshold bitmappings (MTB)[^1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6OIlF5ZXCxf"
   },
   "source": [
    "## Photonics Related Corrections\n",
    "\n",
    "Are applied using enviroment specific calibration data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0eJVxG3XGb4"
   },
   "source": [
    "### Light Falloff Correction\n",
    "\n",
    "Is applied per light source using images with a diffuse plane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from photonics.light_corrections import correct_light_falloff\n",
    "\n",
    "LIGHT_FALLOFF_IMAGES = [read_image(path) for path in LIGHT_FALLOFF_IMAGES_PATHS]\n",
    "\n",
    "TOP_LIGHT_IMAGES = correct_light_falloff(TOP_LIGHT_IMAGES, LIGHT_FALLOFF_IMAGES)\n",
    "BOTTOM_LIGHT_IMAGES = correct_light_falloff(BOTTOM_LIGHT_IMAGES, LIGHT_FALLOFF_IMAGES)\n",
    "\n",
    "del LIGHT_FALLOFF_IMAGES\n",
    "show_images(TOP_LIGHT_IMAGES)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Balance\n",
    "\n",
    "Is applied per light setup using a RGB based white balance when reading the RAW images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTIvFzVcXoru"
   },
   "source": [
    "# Mappings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi_7_jR-YA-y"
   },
   "source": [
    "## Mask\n",
    "from Differently Lit Images with Edge Detection[^2].\n",
    "\n",
    "There are currently three different types of datasets and thus, three different ways to create a mask:\n",
    "\n",
    "1. **classic**: Mask is created using a combination of thresholds and adaptive thresholds as well as skeleton data to preserve fine details and contour detection to organize and sort out detected elements.\n",
    "2. **light table**: Mask is created using a trivial threshold and contour detection to organize and sort out detected elements.\n",
    "3. **frosted glass**: similar to light table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.mask import mask_from_frosted_glass\n",
    "\n",
    "OPACITY_MAP = mask_from_frosted_glass(BOTTOM_LIGHT_IMAGES)\n",
    "OPACITY_MAP[OPACITY_MAP >= 255 / 1.5] = 255\n",
    "OPACITY_MAP[OPACITY_MAP < 255 / 1.5] = 0\n",
    "\n",
    "show_image(OPACITY_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifier Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.identifier import identifier_map\n",
    "\n",
    "IDENTIFIER_MAP, element_count = identifier_map(OPACITY_MAP)\n",
    "\n",
    "print(f\"Element count: {element_count}\")\n",
    "show_image(IDENTIFIER_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kVTKtmqX1MH"
   },
   "source": [
    "## Translucency Mapping\n",
    "from Differently Lit Images with Exposure Fusion[^3].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.translucency import translucency_map\n",
    "\n",
    "TRANSLUCENCY_MAP = translucency_map(BOTTOM_LIGHT_IMAGES)\n",
    "TRANSLUCENCY_MAP[OPACITY_MAP == 0] = [0, 0, 0]\n",
    "\n",
    "show_image(TRANSLUCENCY_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUzHaE8EXzJa"
   },
   "source": [
    "## Albedo Mapping\n",
    "from Differently Lit Images with Exposure Fusion[^3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.albedo import albedo_map\n",
    "\n",
    "ALBEDO_MAP = albedo_map(TOP_LIGHT_IMAGES)\n",
    "ALBEDO_MAP[OPACITY_MAP == 0] = [0, 0, 0]\n",
    "\n",
    "show_image(ALBEDO_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg0gLm9iX4Uf"
   },
   "source": [
    "## Normal Mapping\n",
    "from Differently Lit Images with Photometric Stereo[^4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.normal import normal_map\n",
    "from utilities.image_interpolation import (\n",
    "    edge_extend_image,\n",
    ")\n",
    "\n",
    "NORMAL_MAP = normal_map(TOP_LIGHT_IMAGES)\n",
    "NORMAL_MAP[OPACITY_MAP == 0] = [0, 0, 0]\n",
    "NORMAL_MAP = edge_extend_image(NORMAL_MAP, OPACITY_MAP, 1)\n",
    "\n",
    "show_image(NORMAL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WpW4-QtYKT_"
   },
   "source": [
    "## Roughness Mapping\n",
    "\n",
    "from the Neighbor-Variance of the Normals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.roughness import roughness_map\n",
    "\n",
    "ROUGHNESS_MAP = roughness_map(NORMAL_MAP, OPACITY_MAP)\n",
    "\n",
    "show_image(ROUGHNESS_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyU-GO1BX7it"
   },
   "source": [
    "## Height Mapping\n",
    "\n",
    "The height map is calculated using a 3d integral over the gradients of the normals on the normal map. The gradients are calculated by weighting vertical and horizontal gradient values based on a rotation value. The height map gains accuracy when using multiple rotation values and combining the integrated gradient values to a single height value per pixel.\n",
    "\n",
    "### Calculating the Gradients\n",
    "\n",
    "Given the normal vector $\\vec{n} \\in \\mathbb{R}^{3}$ and a rotation value $r \\in \\mathbb{R}[0,2\\pi]$, the anisotropic gradients are calculated:\n",
    "\n",
    "$$\n",
    "a_h = \\arccos{\\vec{n_x}}, \\hspace{5px} g_l = (1 - \\sin{a_h}) * sgn(a_h - \\frac{\\pi}{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_v = \\arccos{\\vec{n_y}}, \\hspace{5px} g_t = (1 - \\sin{a_v}) * sgn(a_v - \\frac{\\pi}{2})\n",
    "$$\n",
    "\n",
    "This will be calculated for every pixel and for every rotation value.\n",
    "\n",
    "### Calculating the Heights\n",
    "\n",
    "The height values $h(x,y) \\in \\mathbb{R}^{2}, \\ \\ x,y \\in \\mathbb{N}^{0}$ can be calculated by a simple anisotropic cumulative sum over the gradients which converges to an integral over $g(x,y)$:\n",
    "\n",
    "$$\n",
    "h(x_t,y_t) = \\int\\int g(x,y) dydx \\ \\ (x_t,y_t) \\approx \\sum_{x_i=0}^{x_t} g(x_i,y_t)\n",
    "$$\n",
    "\n",
    "This alone is very prone for errors. That’s why the rotation is introduced. When re-calculating the gradient map multiple times with a rotation factor and using that to calculate the height values for every re-calculated gradient map, adding this values together drastically improves the resulting height values:\n",
    "\n",
    "$$\n",
    "h(x_t,y_t) = \\sum_{r=0}^{2\\pi} \\sum_{x_i=0}^{x_t} g(r)(x_i,y_t)\n",
    "$$\n",
    "\n",
    "\n",
    "### Height map normalization\n",
    "To make all the height maps comparable, the height map is not normalized per height map, but per fixed height. In theory this means that all height maps are divided by the same factor, but in practice there is a little caveat to that: Not all datasets are captured with the same focal length. Thus, a slope of 45° in one normal map pixel does not result in the same height on every dataset, because the effective length of a single pixel does not correspond to the same real world length. To counteract this problem another factor is introduced: The texel density (pixels per meter, $\\frac{px}{m}$).\n",
    "\n",
    "The texel density $t$ is calculated given the distance between the camera and the object $\\Delta h$ and the sensor width $w_{c}$ and the focal length $f_{c}$ of the camera. The sensor width and the focal length is used to calculate the angle of view $\\theta_{c}$ of the camera:\n",
    "\n",
    "$$\n",
    "\\theta_{c} = 2 * \\arctan{\\frac{w_{c}}{2f_{c}}}\n",
    "$$\n",
    "\n",
    "Given the angle of view $\\theta_{c}$ and the distance of camera and object $\\Delta h_{c}$ (in other words: the height of the camera), we can calculate the actual width of the scan area (camera viewing width) $a_{w}$:\n",
    "\n",
    "$$\n",
    "a_{w} = 2 \\Delta h_{c} \\tan{\\frac{\\theta_{c}}{2}}\n",
    "$$\n",
    "\n",
    "This and the image width in pixel $i_{w}$ directly leads us to the texel density $t$:\n",
    "\n",
    "$$\n",
    "t = \\frac{i_{w}}{a_{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mappings.height import height_map\n",
    "\n",
    "HEIGHT_MAP = height_map(NORMAL_MAP, OPACITY_MAP, height_divisor=50)\n",
    "\n",
    "show_image(HEIGHT_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kgk69cDBYHZk"
   },
   "source": [
    "## Ambient Occlusion Mapping\n",
    "from Height Mapping with ray-traced baking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: Ward, Greg. \"Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures.\" Journal of graphics tools 8.2 (2003): 17-30.\n",
    "\n",
    "[^2]: Canny, John. \"A computational approach to edge detection.\" IEEE Transactions on pattern analysis and machine intelligence 6 (1986): 679-698.\n",
    "\n",
    "[^3]: Mertens, Tom, Jan Kautz, and Frank Van Reeth. \"Exposure fusion.\" 15th Pacific Conference on Computer Graphics and Applications (PG'07). IEEE, 2007.\n",
    "\n",
    "[^4]: Woodham, Robert J. \"Photometric method for determining surface orientation from multiple images.\" Optical engineering 19.1 (1980): 139-144."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "README.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
